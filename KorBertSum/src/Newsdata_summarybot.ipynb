{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aZ1lg-rIZJak"
   },
   "source": [
    "#0. Default Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uJkdy_8RZGjc"
   },
   "source": [
    "Executed in Colab environment.\n",
    "\n",
    "* ML Framework\n",
    "   - Python 3.7.10\n",
    "   - Pytorch 1.8.1\n",
    "\n",
    "* Hardware\n",
    "   - RAM: 12.7G \n",
    "   - CPU: Intel(R) Xeon(R) CPU @ 2.20GHz (1core)\n",
    "\n",
    "Assumed that data exists like below.\n",
    "you also need a etri openapi key.\n",
    "\n",
    "```\n",
    "/content/gdrive/My Drive/data\n",
    "├── 1_bert_download_001_bert_morp_pytorch.zip\n",
    "```\n",
    "\n",
    "Project Tree (directory only)\n",
    "```\n",
    "/content/KoBertSum\n",
    "├── bert_data\n",
    "├── json_data\n",
    "├── logs\n",
    "├── models\n",
    "├── raw_data\n",
    "├── results\n",
    "├── src\n",
    "│   ├── models\n",
    "│   ├── others\n",
    "│   └── prepro\n",
    "└── urls\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3bYOoWzjYSip"
   },
   "source": [
    "#1. Install dependency packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7p9HcD3e8Ryo",
    "outputId": "0d132884-504f-4029-ef41-d68e812e7b27"
   },
   "outputs": [],
   "source": [
    "# # install bheinzerling's pyrouge\n",
    "# !git clone https://github.com/bheinzerling/pyrouge\n",
    "# %cd pyrouge\n",
    "# !python setup.py -q install\n",
    "# # install missing dependency\n",
    "# !apt install -q libxml-parser-perl\n",
    "# %cd pyrouge\n",
    "# !git clone https://github.com/andersjo/pyrouge.git rouge\n",
    "# !pyrouge_set_rouge_path '/content/pyrouge/rouge/tools/ROUGE-1.5.5'\n",
    "# %cd /content/pyrouge/rouge/tools/ROUGE-1.5.5/data\n",
    "# !mv WordNet-2.0.exc.db WordNet-2.0.exc.db.orig\n",
    "# !perl WordNet-2.0-Exceptions/buildExeptionDB.pl ./WordNet-2.0-Exceptions ./smart_common_words.txt ./WordNet-2.0.exc.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fkQhs-DW_GK9",
    "outputId": "e2a96d5f-72e8-4eea-ee94-a8e0c24eff50"
   },
   "outputs": [],
   "source": [
    "# # 기타 패키지 설치\n",
    "!pip install -q pytorch_pretrained_bert\n",
    "!pip install -q tensorboardX\n",
    "!pip install -q jupyter-dash==0.3.0rc1 dash-bootstrap-components transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Uuo_VZyYvbP"
   },
   "source": [
    "#Google Drive Mount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d3Uzgqx3cLy0"
   },
   "source": [
    "#3. BERT forward propagation workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "M3m9X2IApKrt"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Main training workflow\n",
    "\"\"\"\n",
    "from __future__ import division\n",
    "\n",
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "import signal\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from pytorch_pretrained_bert import BertConfig\n",
    "\n",
    "\n",
    "import distributed\n",
    "from models import data_loader, model_builder\n",
    "from models.data_loader import load_dataset\n",
    "from models.model_builder import Summarizer\n",
    "from tensorboardX import SummaryWriter\n",
    "from models.reporter import ReportMgr\n",
    "from models.stats import Statistics\n",
    "from others.logging import logger\n",
    "# from models.trainer import build_trainer\n",
    "# build_trainer의 dependency package pyrouge.utils가 import되지 않아 직접 셀에 삽입\n",
    "from others.logging import logger, init_logger\n",
    "import easydict\n",
    "\n",
    "args = easydict.EasyDict({\n",
    "    \"encoder\":'classifier',\n",
    "    \"mode\":'summary',\n",
    "    \"bert_data_path\":'../bert_data/korean',\n",
    "    \"model_path\":'../models/bert_classifier_test',\n",
    "    \"bert_model\":'../../korBert_models/001_bert_morp_pytorch',\n",
    "    \"result_path\":'../results/korean',\n",
    "    \"temp_dir\":'.',\n",
    "    \"bert_config_path\":'../../korBert_models/001_bert_morp_pytorch/bert_config.json',\n",
    "    \"batch_size\":1000,\n",
    "    \"use_interval\":True,\n",
    "    \"hidden_size\":128,\n",
    "    \"ff_size\":512,\n",
    "    \"heads\":4,\n",
    "    \"inter_layers\":2,\n",
    "    \"rnn_size\":512,\n",
    "    \"param_init\":0,\n",
    "    \"param_init_glorot\":True,\n",
    "    \"dropout\":0.1,\n",
    "    \"optim\":'adam',\n",
    "    \"lr\":2e-3,\n",
    "    \"report_every\":1,\n",
    "    \"save_checkpoint_steps\":5,\n",
    "    \"block_trigram\":True,\n",
    "    \"recall_eval\":False,\n",
    "    \n",
    "    \"accum_count\":1,\n",
    "    \"world_size\":1,\n",
    "    \"visible_gpus\":'-1',\n",
    "    \"gpu_ranks\":'0',\n",
    "    \"log_file\":'../logs/bert_classifier',\n",
    "    \"test_from\":'../models/bert_classifier_test/model_step_1000.pt'\n",
    "})\n",
    "\n",
    "\n",
    "def build_trainer(args, device_id, model,\n",
    "                  optim):\n",
    "    \"\"\"\n",
    "    Simplify `Trainer` creation based on user `opt`s*\n",
    "    Args:\n",
    "        opt (:obj:`Namespace`): user options (usually from argument parsing)\n",
    "        model (:obj:`onmt.models.NMTModel`): the model to train\n",
    "        fields (dict): dict of fields\n",
    "        optim (:obj:`onmt.utils.Optimizer`): optimizer used during training\n",
    "        data_type (str): string describing the type of data\n",
    "            e.g. \"text\", \"img\", \"audio\"\n",
    "        model_saver(:obj:`onmt.models.ModelSaverBase`): the utility object\n",
    "            used to save the model\n",
    "    \"\"\"\n",
    "    device = \"cpu\" if args.visible_gpus == '-1' else \"cuda\"\n",
    "\n",
    "\n",
    "    grad_accum_count = args.accum_count\n",
    "    n_gpu = args.world_size\n",
    "\n",
    "    if device_id >= 0:\n",
    "        gpu_rank = int(args.gpu_ranks[device_id])\n",
    "    else:\n",
    "        gpu_rank = 0\n",
    "        n_gpu = 0\n",
    "\n",
    "    print('gpu_rank %d' % gpu_rank)\n",
    "\n",
    "    tensorboard_log_dir = args.model_path\n",
    "\n",
    "    writer = SummaryWriter(tensorboard_log_dir, comment=\"Unmt\")\n",
    "\n",
    "    report_manager = ReportMgr(args.report_every, start_time=-1, tensorboard_writer=writer)\n",
    "\n",
    "    trainer = Trainer(args, model, optim, grad_accum_count, n_gpu, gpu_rank, report_manager)\n",
    "\n",
    "    # print(tr)\n",
    "    if (model):\n",
    "        n_params = _tally_parameters(model)\n",
    "        logger.info('* number of parameters: %d' % n_params)\n",
    "\n",
    "    return trainer\n",
    "class Trainer(object):\n",
    "    \"\"\"\n",
    "    Class that controls the training process.\n",
    "\n",
    "    Args:\n",
    "            model(:py:class:`onmt.models.model.NMTModel`): translation model\n",
    "                to train\n",
    "            train_loss(:obj:`onmt.utils.loss.LossComputeBase`):\n",
    "               training loss computation\n",
    "            valid_loss(:obj:`onmt.utils.loss.LossComputeBase`):\n",
    "               training loss computation\n",
    "            optim(:obj:`onmt.utils.optimizers.Optimizer`):\n",
    "               the optimizer responsible for update\n",
    "            trunc_size(int): length of truncated back propagation through time\n",
    "            shard_size(int): compute loss in shards of this size for efficiency\n",
    "            data_type(string): type of the source input: [text|img|audio]\n",
    "            norm_method(string): normalization methods: [sents|tokens]\n",
    "            grad_accum_count(int): accumulate gradients this many times.\n",
    "            report_manager(:obj:`onmt.utils.ReportMgrBase`):\n",
    "                the object that creates reports, or None\n",
    "            model_saver(:obj:`onmt.models.ModelSaverBase`): the saver is\n",
    "                used to save a checkpoint.\n",
    "                Thus nothing will be saved if this parameter is None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,  args, model,  optim,\n",
    "                  grad_accum_count=1, n_gpu=1, gpu_rank=1,\n",
    "                  report_manager=None):\n",
    "        # Basic attributes.\n",
    "        self.args = args\n",
    "        self.save_checkpoint_steps = args.save_checkpoint_steps\n",
    "        self.model = model\n",
    "        self.optim = optim\n",
    "        self.grad_accum_count = grad_accum_count\n",
    "        self.n_gpu = n_gpu\n",
    "        self.gpu_rank = gpu_rank\n",
    "        self.report_manager = report_manager\n",
    "\n",
    "        self.loss = torch.nn.BCELoss(reduction='none')\n",
    "        assert grad_accum_count > 0\n",
    "        # Set model in training mode.\n",
    "        if (model):\n",
    "            self.model.train()\n",
    "\n",
    "    def summary(self, test_iter, step, cal_lead=False, cal_oracle=False):\n",
    "        \"\"\" Validate model.\n",
    "            valid_iter: validate data iterator\n",
    "        Returns:\n",
    "            :obj:`nmt.Statistics`: validation loss statistics\n",
    "        \"\"\"\n",
    "        # Set model in validating mode.\n",
    "        def _get_ngrams(n, text):\n",
    "            ngram_set = set()\n",
    "            text_length = len(text)\n",
    "            max_index_ngram_start = text_length - n\n",
    "            for i in range(max_index_ngram_start + 1):\n",
    "                ngram_set.add(tuple(text[i:i + n]))\n",
    "            return ngram_set\n",
    "\n",
    "        def _block_tri(c, p):\n",
    "            tri_c = _get_ngrams(3, c.split())\n",
    "            for s in p:\n",
    "                tri_s = _get_ngrams(3, s.split())\n",
    "                if len(tri_c.intersection(tri_s))>0:\n",
    "                    return True\n",
    "            return False\n",
    "\n",
    "        if (not cal_lead and not cal_oracle):\n",
    "            self.model.eval()\n",
    "        stats = Statistics()\n",
    "\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in test_iter:\n",
    "                src = batch.src\n",
    "                labels = batch.labels\n",
    "                segs = batch.segs\n",
    "                clss = batch.clss\n",
    "                mask = batch.mask\n",
    "                mask_cls = batch.mask_cls\n",
    "\n",
    "\n",
    "                gold = []\n",
    "                pred = []\n",
    "\n",
    "                if (cal_lead):\n",
    "                    selected_ids = [list(range(batch.clss.size(1)))] * batch.batch_size\n",
    "                elif (cal_oracle):\n",
    "                    selected_ids = [[j for j in range(batch.clss.size(1)) if labels[i][j] == 1] for i in\n",
    "                                    range(batch.batch_size)]\n",
    "                else:\n",
    "                    sent_scores, mask = self.model(src, segs, clss, mask, mask_cls)\n",
    "\n",
    "                    # loss = self.loss(sent_scores, labels.float())\n",
    "                    # loss = (loss * mask.float()).sum()\n",
    "                    # batch_stats = Statistics(float(loss.cpu().data.numpy()), len(labels))\n",
    "                    # stats.update(batch_stats)\n",
    "\n",
    "                    sent_scores = sent_scores + mask.float()\n",
    "                    sent_scores = sent_scores.cpu().data.numpy()\n",
    "                    selected_ids = np.argsort(-sent_scores, 1)\n",
    "                # selected_ids = np.sort(selected_ids,1)\n",
    "                \n",
    "\n",
    "        return selected_ids\n",
    "\n",
    "\n",
    "    def _gradient_accumulation(self, true_batchs, normalization, total_stats,\n",
    "                               report_stats):\n",
    "        if self.grad_accum_count > 1:\n",
    "            self.model.zero_grad()\n",
    "\n",
    "        for batch in true_batchs:\n",
    "            if self.grad_accum_count == 1:\n",
    "                self.model.zero_grad()\n",
    "\n",
    "            src = batch.src\n",
    "            labels = batch.labels\n",
    "            segs = batch.segs\n",
    "            clss = batch.clss\n",
    "            mask = batch.mask\n",
    "            mask_cls = batch.mask_cls\n",
    "\n",
    "            sent_scores, mask = self.model(src, segs, clss, mask, mask_cls)\n",
    "\n",
    "            loss = self.loss(sent_scores, labels.float())\n",
    "            loss = (loss*mask.float()).sum()\n",
    "            (loss/loss.numel()).backward()\n",
    "            # loss.div(float(normalization)).backward()\n",
    "\n",
    "            batch_stats = Statistics(float(loss.cpu().data.numpy()), normalization)\n",
    "\n",
    "\n",
    "            total_stats.update(batch_stats)\n",
    "            report_stats.update(batch_stats)\n",
    "\n",
    "            # 4. Update the parameters and statistics.\n",
    "            if self.grad_accum_count == 1:\n",
    "                # Multi GPU gradient gather\n",
    "                if self.n_gpu > 1:\n",
    "                    grads = [p.grad.data for p in self.model.parameters()\n",
    "                             if p.requires_grad\n",
    "                             and p.grad is not None]\n",
    "                    distributed.all_reduce_and_rescale_tensors(\n",
    "                        grads, float(1))\n",
    "                self.optim.step()\n",
    "\n",
    "        # in case of multi step gradient accumulation,\n",
    "        # update only after accum batches\n",
    "        if self.grad_accum_count > 1:\n",
    "            if self.n_gpu > 1:\n",
    "                grads = [p.grad.data for p in self.model.parameters()\n",
    "                         if p.requires_grad\n",
    "                         and p.grad is not None]\n",
    "                distributed.all_reduce_and_rescale_tensors(\n",
    "                    grads, float(1))\n",
    "            self.optim.step()\n",
    "\n",
    "    def _save(self, step):\n",
    "        real_model = self.model\n",
    "        # real_generator = (self.generator.module\n",
    "        #                   if isinstance(self.generator, torch.nn.DataParallel)\n",
    "        #                   else self.generator)\n",
    "\n",
    "        model_state_dict = real_model.state_dict()\n",
    "        # generator_state_dict = real_generator.state_dict()\n",
    "        checkpoint = {\n",
    "            'model': model_state_dict,\n",
    "            # 'generator': generator_state_dict,\n",
    "            'opt': self.args,\n",
    "            'optim': self.optim,\n",
    "        }\n",
    "        checkpoint_path = os.path.join(self.args.model_path, 'model_step_%d.pt' % step)\n",
    "        logger.info(\"Saving checkpoint %s\" % checkpoint_path)\n",
    "        # checkpoint_path = '%s_step_%d.pt' % (FLAGS.model_path, step)\n",
    "        if (not os.path.exists(checkpoint_path)):\n",
    "            torch.save(checkpoint, checkpoint_path)\n",
    "            return checkpoint, checkpoint_path\n",
    "\n",
    "    def _start_report_manager(self, start_time=None):\n",
    "        \"\"\"\n",
    "        Simple function to start report manager (if any)\n",
    "        \"\"\"\n",
    "        if self.report_manager is not None:\n",
    "            if start_time is None:\n",
    "                self.report_manager.start()\n",
    "            else:\n",
    "                self.report_manager.start_time = start_time\n",
    "\n",
    "    def _maybe_gather_stats(self, stat):\n",
    "        \"\"\"\n",
    "        Gather statistics in multi-processes cases\n",
    "\n",
    "        Args:\n",
    "            stat(:obj:onmt.utils.Statistics): a Statistics object to gather\n",
    "                or None (it returns None in this case)\n",
    "\n",
    "        Returns:\n",
    "            stat: the updated (or unchanged) stat object\n",
    "        \"\"\"\n",
    "        if stat is not None and self.n_gpu > 1:\n",
    "            return Statistics.all_gather_stats(stat)\n",
    "        return stat\n",
    "\n",
    "    def _maybe_report_training(self, step, num_steps, learning_rate,\n",
    "                               report_stats):\n",
    "        \"\"\"\n",
    "        Simple function to report training stats (if report_manager is set)\n",
    "        see `onmt.utils.ReportManagerBase.report_training` for doc\n",
    "        \"\"\"\n",
    "        if self.report_manager is not None:\n",
    "            return self.report_manager.report_training(\n",
    "                step, num_steps, learning_rate, report_stats,\n",
    "                multigpu=self.n_gpu > 1)\n",
    "\n",
    "    def _report_step(self, learning_rate, step, train_stats=None,\n",
    "                     valid_stats=None):\n",
    "        \"\"\"\n",
    "        Simple function to report stats (if report_manager is set)\n",
    "        see `onmt.utils.ReportManagerBase.report_step` for doc\n",
    "        \"\"\"\n",
    "        if self.report_manager is not None:\n",
    "            return self.report_manager.report_step(\n",
    "                learning_rate, step, train_stats=train_stats,\n",
    "                valid_stats=valid_stats)\n",
    "\n",
    "    def _maybe_save(self, step):\n",
    "        \"\"\"\n",
    "        Save the model if a model saver is set\n",
    "        \"\"\"\n",
    "        if self.model_saver is not None:\n",
    "            self.model_saver.maybe_save(step)\n",
    "\n",
    "def summary(args, b_list, device_id, pt, step):\n",
    "\n",
    "    device = \"cpu\" if args.visible_gpus == '-1' else \"cuda\"\n",
    "    if (pt != ''):\n",
    "        test_from = pt\n",
    "    else:\n",
    "        test_from = args.test_from\n",
    "    logger.info('Loading checkpoint from %s' % test_from)\n",
    "    checkpoint = torch.load(test_from, map_location=lambda storage, loc: storage)\n",
    "    opt = vars(checkpoint['opt'])\n",
    "    for k in opt.keys():\n",
    "        if (k in model_flags):\n",
    "            setattr(args, k, opt[k])\n",
    "    print(args)\n",
    "\n",
    "    config = BertConfig.from_json_file(args.bert_config_path)\n",
    "    model = Summarizer(args, device, load_pretrained_bert=False, bert_config = config)\n",
    "    model.load_cp(checkpoint)\n",
    "    model.eval()\n",
    "\n",
    "    test_iter =data_loader.Dataloader(args, _lazy_dataset_loader(b_list),\n",
    "                                  args.batch_size, device,\n",
    "                                  shuffle=False, is_test=True)\n",
    "    trainer = build_trainer(args, device_id, model, None)\n",
    "    result = trainer.summary(test_iter,step)\n",
    "    return result\n",
    "def _tally_parameters(model):\n",
    "    n_params = sum([p.nelement() for p in model.parameters()])\n",
    "    return n_params\n",
    "\n",
    "args.gpu_ranks = [int(i) for i in args.gpu_ranks.split(',')]\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.visible_gpus\n",
    "\n",
    "init_logger(args.log_file)\n",
    "device = \"cpu\" if args.visible_gpus == '-1' else \"cuda\"\n",
    "device_id = 0 if device == \"cuda\" else -1\n",
    "model_flags = ['hidden_size', 'ff_size', 'heads', 'inter_layers','encoder','ff_actv', 'use_interval','rnn_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xji5mhCxcWcg"
   },
   "source": [
    "#4. Input data morp-tokenization workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Diau6__Vhhuo"
   },
   "source": [
    "##your openapi_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "moouXLWzclio"
   },
   "outputs": [],
   "source": [
    "openapi_key = '220c9e91-8f30-442a-a8a5-5dffc2aab0c6'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ifDaOjcThpEr"
   },
   "source": [
    "##workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "UAnJ4R8b0hYd"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import urllib3\n",
    "from glob import glob\n",
    "import collections\n",
    "import six\n",
    "import gc\n",
    "\n",
    "def do_lang ( openapi_key, text ) :\n",
    "    openApiURL = \"http://aiopen.etri.re.kr:8000/WiseNLU\"\n",
    "    requestJson = { \"access_key\": openapi_key, \"argument\": { \"text\": text, \"analysis_code\": \"morp\" } }\n",
    "    http = urllib3.PoolManager()\n",
    "    response = http.request( \"POST\", openApiURL, headers={\"Content-Type\": \"application/json; charset=UTF-8\"}, body=json.dumps(requestJson))\n",
    "    \n",
    "    json_data = json.loads(response.data.decode('utf-8'))\n",
    "    json_result = json_data[\"result\"]\n",
    "    \n",
    "    if json_result == -1:\n",
    "        json_reason = json_data[\"reason\"]\n",
    "        if \"Invalid Access Key\" in json_reason:\n",
    "            logger.info(json_reason)\n",
    "            logger.info(\"Please check the openapi access key.\")\n",
    "            sys.exit()\n",
    "        return \"openapi error - \" + json_reason\n",
    "    else:\n",
    "        json_data = json.loads(response.data.decode('utf-8'))\n",
    "    \n",
    "        json_return_obj = json_data[\"return_object\"]\n",
    "        \n",
    "        return_result = \"\"\n",
    "        json_sentence = json_return_obj[\"sentence\"]\n",
    "        for json_morp in json_sentence:\n",
    "            for morp in json_morp[\"morp\"]:\n",
    "                return_result = return_result+str(morp[\"lemma\"])+\"/\"+str(morp[\"type\"])+\" \"\n",
    "\n",
    "        return return_result\n",
    "class BertData():\n",
    "    def __init__(self, vocab_file_path):\n",
    "        self.tokenizer = Tokenizer(vocab_file_path)\n",
    "        self.sep_vid = self.tokenizer.vocab['[SEP]']\n",
    "        self.cls_vid = self.tokenizer.vocab['[CLS]']\n",
    "        self.pad_vid = self.tokenizer.vocab['[PAD]']\n",
    "\n",
    "    def preprocess(self, src):\n",
    "\n",
    "        if (len(src) == 0):\n",
    "            return None\n",
    "\n",
    "        original_src_txt = [''.join(s) for s in src]\n",
    "\n",
    "\n",
    "        idxs = [i for i, s in enumerate(src) if (len(s) > 0)]\n",
    "\n",
    "        src = [src[i][:20000] for i in idxs]\n",
    "        src = src[:10000]\n",
    "\n",
    "        if (len(src) < 3):\n",
    "            return None\n",
    "\n",
    "        src_txt = [''.join(sent) for sent in src]\n",
    "        text = ' [SEP] [CLS] '.join(src_txt)\n",
    "        src_subtokens = text.split(' ')\n",
    "        src_subtokens = src_subtokens[:510]\n",
    "        src_subtokens = ['[CLS]'] + src_subtokens + ['[SEP]']\n",
    "\n",
    "        src_subtoken_idxs = self.tokenizer.convert_tokens_to_ids(src_subtokens)\n",
    "        _segs = [-1] + [i for i, t in enumerate(src_subtoken_idxs) if t == self.sep_vid]\n",
    "        segs = [_segs[i] - _segs[i - 1] for i in range(1, len(_segs))]\n",
    "        segments_ids = []\n",
    "        for i, s in enumerate(segs):\n",
    "            if (i % 2 == 0):\n",
    "                segments_ids += s * [0]\n",
    "            else:\n",
    "                segments_ids += s * [1]\n",
    "        cls_ids = [i for i, t in enumerate(src_subtoken_idxs) if t == self.cls_vid]\n",
    "        labels = None\n",
    "        tgt_txt = None\n",
    "        src_txt = [original_src_txt[i] for i in idxs]\n",
    "        return src_subtoken_idxs, labels, segments_ids, cls_ids, src_txt, tgt_txt\n",
    "def convert_to_unicode(text):\n",
    "    \"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"\n",
    "    if six.PY3:\n",
    "        if isinstance(text, str):\n",
    "            return text\n",
    "        elif isinstance(text, bytes):\n",
    "            return text.decode(\"utf-8\", \"ignore\")\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "    elif six.PY2:\n",
    "        if isinstance(text, str):\n",
    "            return text.decode(\"utf-8\", \"ignore\")\n",
    "        elif isinstance(text, unicode):\n",
    "            return text\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "    else:\n",
    "        raise ValueError(\"Not running on Python2 or Python 3?\")\n",
    "class Tokenizer(object):\n",
    "    def __init__(self, vocab_file_path):\n",
    "        self.vocab_file_path = vocab_file_path\n",
    "        \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n",
    "        vocab = collections.OrderedDict()\n",
    "        index = 0\n",
    "        with open(self.vocab_file_path, \"r\", encoding='utf-8') as reader:\n",
    "\n",
    "            while True:\n",
    "                token = convert_to_unicode(reader.readline())\n",
    "                if not token:\n",
    "                    break\n",
    "\n",
    "          ### joonho.lim @ 2019-03-15\n",
    "                if token.find('n_iters=') == 0 or token.find('max_length=') == 0 :\n",
    "\n",
    "                    continue\n",
    "                token = token.split('\\t')[0].strip('_')\n",
    "\n",
    "                token = token.strip()\n",
    "                vocab[token] = index\n",
    "                index += 1\n",
    "        self.vocab = vocab\n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        \"\"\"Converts a sequence of tokens into ids using the vocab.\"\"\"\n",
    "        ids = []\n",
    "        for token in tokens:\n",
    "            try:\n",
    "                ids.append(self.vocab[token])\n",
    "            except:\n",
    "                ids.append(1)\n",
    "        if len(ids) > 10000:\n",
    "            raise ValueError(\n",
    "                \"Token indices sequence length is longer than the specified maximum \"\n",
    "                \" sequence length for this BERT model ({} > {}). Running this\"\n",
    "                \" sequence through BERT will result in indexing errors\".format(len(ids), 10000)\n",
    "            )\n",
    "        return ids\n",
    "def _lazy_dataset_loader(pt_file):\n",
    "    \n",
    "    dataset = pt_file\n",
    "    \n",
    "    yield dataset\n",
    "def News_to_input(text, openapi_key):\n",
    "    newstemp = do_lang(openapi_key,text)\n",
    "    news = newstemp.split(' ./SF ')[:-1]\n",
    "    bertdata = BertData('../../korBert_models/001_bert_morp_pytorch/vocab.korean_morp.list')\n",
    "    tmp = bertdata.preprocess(news)\n",
    "    b_data_dict = {\"src\":tmp[0],\n",
    "               \"labels\":[0,1,2],\n",
    "               \"segs\":tmp[2],\n",
    "               \"clss\":tmp[3],\n",
    "               \"src_txt\":tmp[4],\n",
    "               \"tgt_txt\":'hehe'}\n",
    "    b_list = []\n",
    "    b_list.append(b_data_dict) \n",
    "    return b_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"\"\"저는 올해 초등학교 6학년인 남자아이와 4학년인 여자아이를 자녀로 두고 있는 42살의 14년차 전업주부 OOO입니다.\n",
    "미국인과 결혼하신 이모 덕에 어려서부터 영어에 대한 관심이 많았고 영어공부도 좋아했습니다. 관련해서 더 깊이있게 공부하고 싶었지만 고3 시절 가정형편이 좋지 못한 사정으로 대학진학을 통한 공부의 꿈을 미루고 바로 취직을 하게 됐습니다. 이후 전업주부가 되기 전까지는 백화점에서 5년간 근무하며 판매왕, 친절왕으로 여러 번 뽑혔을 정도로 제 성격은 매사에 끈기가 있고 적극적이며, 주위 사람에게 친절한 편입니다.\n",
    "하루 대부분의 시간을 주로 남편과 아이들을 위해 사용하지만 평일 낮 시간, 주말시간을 활용해 틈틈이 새로운 일을 배우고 자기계발을 하려고 노력합니다. 이 결과 작년에는 앙금플라워자격증, 올해에는 네일아트자격증을 취득하였습니다. 아주 대단한 자격증도 아니고 당장 어떤 가게를 차릴 것도 아니지만 무언가 새로운 일에 도전하고 성취감을 느끼는 걸 즐깁니다.\n",
    "14년간 아내로서 엄마로서 최선을 다해 열심히 살아오면서도 항상 배움, 그중에서도 체계적인 학습에 대한 아쉬움이 있었습니다. 또한 주부이지만 성장하는 아이들에게 멋진 엄마의 모습을 보여주고 싶습니다.\n",
    "제 환경이나 상황이 한 번에 긴 시간을 공부하기는 어렵겠지만 낮에는 아이들과 함께 공부하고, 아이들이 잠든 후 밤 시간을 이용해 짬짬이 공부하려고 합니다. 또한 주말에는 사이버외대에서 진행하는 특강이나 모임에도 적극적으로 참석해서 다른 학생들과 함께 공부하고, 쉽지는 않겠지만 해외 문화탐방과 같은 문화체험도 참가해 보고 싶습니다.\n",
    "사이버외대를 졸업한 후에 여건이 된다면 대학원에도 진학해 보고 싶습니다. 학부과정에서 키운 언어에 대한 실력과 전문성을 바탕으로 대학원에서는 효과적으로 영어를 가르칠 수 있는 교육법(교수법)에 대해 공부할 계획입니다. 또한 향후 가능하다면 어린이, 학생을 대상으로 영어를 가르치는 봉사활동도 해보고 싶습니다.\n",
    "원어민 교수님을 포함한 외국인과 거리낌 없이 소통하고 즐길 수 있는 가까운 미래 제 모습을 상상하며 열심히 하겠습니다.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 2, 5, 3, 4, 9, 7, 6, 1, 8]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6nhubPG1c5zt"
   },
   "source": [
    "#5. html for SummaryBot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['저는 올해 초등학교 6학년인 남자아이와 4학년인 여자아이를 자녀로 두고 있는 42살의 14년차 전업주부 OOO입니다',\n",
       " '미국인과 결혼하신 이모 덕에 어려서부터 영어에 대한 관심이 많았고 영어공부도 좋아했습니다',\n",
       " '관련해서 더 깊이있게 공부하고 싶었지만 고3 시절 가정형편이 좋지 못한 사정으로 대학진학을 통한 공부의 꿈을 미루고 바로 취직을 하게 됐습니다',\n",
       " '이후 전업주부가 되기 전까지는 백화점에서 5년간 근무하며 판매왕, 친절왕으로 여러 번 뽑혔을 정도로 제 성격은 매사에 끈기가 있고 적극적이며, 주위 사람에게 친절한 편입니다',\n",
       " '하루 대부분의 시간을 주로 남편과 아이들을 위해 사용하지만 평일 낮 시간, 주말시간을 활용해 틈틈이 새로운 일을 배우고 자기계발을 하려고 노력합니다',\n",
       " '이 결과 작년에는 앙금플라워자격증, 올해에는 네일아트자격증을 취득하였습니다',\n",
       " '아주 대단한 자격증도 아니고 당장 어떤 가게를 차릴 것도 아니지만 무언가 새로운 일에 도전하고 성취감을 느끼는 걸 즐깁니다',\n",
       " '14년간 아내로서 엄마로서 최선을 다해 열심히 살아오면서도 항상 배움, 그중에서도 체계적인 학습에 대한 아쉬움이 있었습니다',\n",
       " '또한 주부이지만 성장하는 아이들에게 멋진 엄마의 모습을 보여주고 싶습니다',\n",
       " '제 환경이나 상황이 한 번에 긴 시간을 공부하기는 어렵겠지만 낮에는 아이들과 함께 공부하고, 아이들이 잠든 후 밤 시간을 이용해 짬짬이 공부하려고 합니다',\n",
       " '또한 주말에는 사이버외대에서 진행하는 특강이나 모임에도 적극적으로 참석해서 다른 학생들과 함께 공부하고, 쉽지는 않겠지만 해외 문화탐방과 같은 문화체험도 참가해 보고 싶습니다',\n",
       " '사이버외대를 졸업한 후에 여건이 된다면 대학원에도 진학해 보고 싶습니다',\n",
       " '학부과정에서 키운 언어에 대한 실력과 전문성을 바탕으로 대학원에서는 효과적으로 영어를 가르칠 수 있는 교육법(교수법)에 대해 공부할 계획입니다',\n",
       " '또한 향후 가능하다면 어린이, 학생을 대상으로 영어를 가르치는 봉사활동도 해보고 싶습니다',\n",
       " '원어민 교수님을 포함한 외국인과 거리낌 없이 소통하고 즐길 수 있는 가까운 미래 제 모습을 상상하며 열심히 하겠습니다',\n",
       " '']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda x: x.strip(),test.split('.')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-11-26 16:25:34,102 INFO] Loading checkpoint from ../models/bert_classifier_test/model_step_1000.pt\n",
      "[2021-11-26 16:25:34,851 INFO] loading archive file ../../korBert_models/001_bert_morp_pytorch\n",
      "[2021-11-26 16:25:34,852 INFO] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30349\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'encoder': 'classifier', 'mode': 'summary', 'bert_data_path': '../bert_data/korean', 'model_path': '../models/bert_classifier_test', 'bert_model': '../../korBert_models/001_bert_morp_pytorch', 'result_path': '../results/korean', 'temp_dir': '.', 'bert_config_path': '../../korBert_models/001_bert_morp_pytorch/bert_config.json', 'batch_size': 1000, 'use_interval': True, 'hidden_size': 128, 'ff_size': 512, 'heads': 4, 'inter_layers': 2, 'rnn_size': 512, 'param_init': 0, 'param_init_glorot': True, 'dropout': 0.1, 'optim': 'adam', 'lr': 0.002, 'report_every': 1, 'save_checkpoint_steps': 5, 'block_trigram': True, 'recall_eval': False, 'accum_count': 1, 'world_size': 1, 'visible_gpus': '-1', 'gpu_ranks': [0], 'log_file': '../logs/bert_classifier', 'test_from': '../models/bert_classifier_test/model_step_1000.pt'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-11-26 16:25:36,817 INFO] * number of parameters: 109350145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu_rank 0\n"
     ]
    }
   ],
   "source": [
    "bot_input_ids = News_to_input(test, openapi_key)        \n",
    "chat_history_ids = summary(args, bot_input_ids, -1, '', None)\n",
    "pred_lst = list(chat_history_ids[0][:3])\n",
    "final_text = ''\n",
    "for i,a in enumerate(list(map(lambda x: x.strip(),test.split('.')))):\n",
    "    if i in pred_lst:\n",
    "        final_text = final_text+a+'. '\n",
    "chat_history = test + '<token>' +final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n이재명 더불어민주당 대선후보가 과거 조카가 저지른 살인 사건에 대한\\u2008변호를 맡았던 일이 표면화되자 26일 \"가슴 아픈 일이고 다시 한 번 사과드린다\"고 밝혔다.\\n\\n이 후보는 이날 기자들과 만나 \"모든 범죄의 피해자들은 억울한 것\"이라면서도 \"그 점에 대해서 제가 멀다고 할 수도 없는 친척의 일을 제가 처리하였는데 아쉬움, 억울함에 대해서 제가 말씀드렸다\"고 했다. 이 후보는 거듭 \"안타까운 일\", \"마음 아픈 일\"이라면서 \"다시 한 번 사과드린다\"고 말했다.\\n\\n앞서 이 후보는 유엔의 여성 폭력 추방의 날이었던 지난 25일 자신의 페이스북을 통해 \"제 일가 중 일인이 과거 데이트 폭력 중범죄를 저질렀는데, 그 가족들이 변호사를 선임할 형편이 못돼 일가 중 유일한 변호사인 제가 변론을 맡을 수밖에 없었다\"며 \"피해자와 유가족분들에게 깊은 위로와 사과의 말씀을 드린다\"고 했다. 이어 \"제게도 이 사건은 평생 지우지 못할 고통스런 기억이다. 어떤 말로도 피해자와 유족들의 상처가 아물지 않을 것\"이라며 데이트 폭력에 대한 특별 대책을 강구하겠다고 밝힌 바 있다.\\n\\n이 후보가 언급한 \\'데이트 폭력 중범죄\\'는 지난 2006년 5월 서울 강동구에서 일어난 \\'모녀 살인 사건\\'이다. 당시 이 후보 조카 김 모씨는 헤어진 여자친구가 살던 집에 찾아가 전 여자친구와 그녀의 어머니를 흉기로 여러 차례 찔러 살해해 대법원에서 무기징역을 확정 받았다. 이 후보는 가해자인 조카의 1, 2심 재판 변호를 맡았고, 당시 조카를 변호하며 \\'충동 조절 능력 저하로 심신미약 상태에 있었다\\'며 감형을 주장한 것으로 전해졌다.\\n\\n해당 사건으로 딸과 아내를 잃은 A씨는 이날 <문화일보>와의 인터뷰에서 \"15년이 지났지만 그 일만 생각하면 심장이 저릿저릿하다\"며 \"사건 당시에도 사과는 없었고, 현재까지도 이 후보 일가 측으로부터 사과 연락이 온 적이 단 한 번도 없다\"고 했다. A씨는 \"우리는 평생을 고통 속에서 살아가야 하는데, 이제 와서 예전 일을 끄집어내 보란 듯 얘기하는데 참 뻔뻔하다\"며 \"갑자기 TV에서 사과 비슷하게 하는 모습을 보니, 그저 채널을 돌릴 수밖에 없었다\"고 울분을 토로했다.\\n\\n이 후보가 조카를 변호하며 \\'심신미약으로 인한 감형\\'을 주장한 것에 대해 \"내 딸의 남자친구였던 그 놈은 정신 이상은 전혀 없는 사람이었다\"면서 \"뻔뻔하게 심신미약, 정신이상을 주장했다는 게 참…\"이라며 말을 잇지 못했다.\\n\\n국민의힘 선대위 전주혜 대변인은 논평을 통해 \"흉악 살인 범죄를 변호하면서 충동 조절 능력 저하나 심신 미약 상태를 주장한 사람이 어떻게 피해자의 입장을 헤아릴 수 있겠는가\"라며 \"국가지도자라면 마땅히 가져야 할 약자에 대한 기본 인식과 공감 능력의 심각한 부재\"라고 비판했다.\\n'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['저는 올해 초등학교 6학년인 남자아이와 4학년인 여자아이를 자녀로 두고 있는 42살의 14년차 전업주부 OOO입니다. 이 결과 작년에는 앙금플라워자격증, 올해에는 네일아트자격증을 취득하였습니다. 14년간 아내로서 엄마로서 최선을 다해 열심히 살아오면서도 항상 배움, 그중에서도 체계적인 학습에 대한 아쉬움이 있었습니다.']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_text.strip().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n김종인 전 국민의힘 비상대책위원장이 26일 윤석열 국민의힘 대선후보 총괄선거대책위원장을 맡지 않겠다는 의사를 밝혔다. 김병준 전 비상대책위원장이 상임선대위원장을 수락한 직후다.\\n\\n김병준 전 비상대책위원장은 이날 국민의힘 당사에서 기자간담회를 열고 상임선대위원장을 맡기로 했다고 밝혔다. 김병준 전 위원장은 \"상임선대위원장직을 열심히 할 생각\"이라고 밝혔다. 그는 \"우리 정치가 시대에 뒤떨어져 과감히 바꿀 때가 됐다고 생각했는데, 그런 일을 하겠다는 분을 혼자 뛰게 둔다는 게 우리 모두의 도리가 아니라는 생각이 들었다. 무엇이든 돕겠다는 생각\"이라고 밝혔다. 그동안 김종인 전 위원장과 불화로 돌던 상임선대위원장 사퇴설을 일축한 것이다.\\n\\n김병준 전 위원장이 상임선대위원장직을 수락한 직후 김종인 전 위원장은 \\'총괄선대위원장을 고려하지 않겠다\\'는 의사를 내비쳤다. 김종인 전 위원장은 서울 종로구 사무실에서 나온 뒤 \\'김병준 상임위원장이 열심히 하겠다고 밝혔는데, 총괄선대위원장직은 고려하지 않는 건가\\'라고 묻는 기자의 질문에 고개를 끄덕였다. \\'끄덕이신 거 맞느냐\\'고 재차 묻는 말에도 김종인 전 위원장은 다시 고개를 끄덕였다.\\n\\n그밖에 선대위 구성과 관련한 질문에 김종인 전 위원장은 \"나는 아무 전달을 받은 게 없다. 할 말이 없다는데 자꾸 물어보느냐\"고 답했다.\\n\\n김종인 전 위원장은 그동안 김병준 상임선대위원장 인선을 철회하는 조건으로 총괄선대위원장을 맡겠다는 의사를 밝혀왔다. 자신의 의사가 관철되지 않는 상황에서 총괄선대위원장을 맡지 않기로 결심한 것으로 해석된다.\\n\\n이로써 윤석열 후보 선대위는 총괄선대위원장 없이 김병준 상임총대위원장-김한길 새시대준비위원장 \\'투 김\\' 체제로 시작하거나, 새로운 총괄선대위원장을 영입할 가능성도 생겼다.\\n\\n이준석 국민의힘 대표는 김종인 전 위원장이 합류하지 못할 경우 다른 인사를 총괄선대위원장으로 세워야 한다는 뜻을 밝히기도 했다. 이 대표는 지난 25일 KBS라디오 \\'최경영의 최강시사\\'에 출연해 \"윤석열 후보에게 \\'만약 김종인 위원장을 모시지 못하는 상황이면 김병준 위원장을 포함해서 다른 인사를 총괄선대위원장으로 세워도 좋다\\'는 제 의사를 전달한 적이 있다\"고 말했다.\\n<token>\\n김종인 전 국민의힘 비상대책위원장이 26일 윤석열 국민의힘 대선후보 총괄선거대책위원장을 맡지 않겠다는 의사를 밝혔다. 무엇이든 돕겠다는 생각\"이라고 밝혔다. 김종인 전 위원장은 서울 종로구 사무실에서 나온 뒤 \\'김병준 상임위원장이 열심히 하겠다고 밝혔는데, 총괄선대위원장직은 고려하지 않는 건가\\'라고 묻는 기자의 질문에 고개를 끄덕였다. '"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://ftp.daumkakao.com/pypi/simple\n",
      "Requirement already satisfied: dash in /home/ubuntu/anaconda3/envs/jh_torch37/lib/python3.7/site-packages (2.0.0)\n",
      "Requirement already satisfied: dash-html-components==2.0.0 in /home/ubuntu/anaconda3/envs/jh_torch37/lib/python3.7/site-packages (from dash) (2.0.0)\n",
      "Requirement already satisfied: dash-table==5.0.0 in /home/ubuntu/anaconda3/envs/jh_torch37/lib/python3.7/site-packages (from dash) (5.0.0)\n",
      "Requirement already satisfied: flask-compress in /home/ubuntu/anaconda3/envs/jh_torch37/lib/python3.7/site-packages (from dash) (1.10.1)\n",
      "Requirement already satisfied: plotly>=5.0.0 in /home/ubuntu/anaconda3/envs/jh_torch37/lib/python3.7/site-packages (from dash) (5.4.0)\n",
      "Requirement already satisfied: dash-core-components==2.0.0 in /home/ubuntu/anaconda3/envs/jh_torch37/lib/python3.7/site-packages (from dash) (2.0.0)\n",
      "Requirement already satisfied: Flask>=1.0.4 in /home/ubuntu/anaconda3/envs/jh_torch37/lib/python3.7/site-packages (from dash) (2.0.2)\n",
      "Requirement already satisfied: click>=7.1.2 in /home/ubuntu/anaconda3/envs/jh_torch37/lib/python3.7/site-packages (from Flask>=1.0.4->dash) (8.0.3)\n",
      "Requirement already satisfied: Werkzeug>=2.0 in /home/ubuntu/anaconda3/envs/jh_torch37/lib/python3.7/site-packages (from Flask>=1.0.4->dash) (2.0.2)\n",
      "Requirement already satisfied: Jinja2>=3.0 in /home/ubuntu/anaconda3/envs/jh_torch37/lib/python3.7/site-packages (from Flask>=1.0.4->dash) (3.0.3)\n",
      "Requirement already satisfied: itsdangerous>=2.0 in /home/ubuntu/anaconda3/envs/jh_torch37/lib/python3.7/site-packages (from Flask>=1.0.4->dash) (2.0.1)\n",
      "Requirement already satisfied: importlib-metadata in /home/ubuntu/anaconda3/envs/jh_torch37/lib/python3.7/site-packages (from click>=7.1.2->Flask>=1.0.4->dash) (4.8.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ubuntu/anaconda3/envs/jh_torch37/lib/python3.7/site-packages (from Jinja2>=3.0->Flask>=1.0.4->dash) (2.0.1)\n",
      "Requirement already satisfied: six in /home/ubuntu/anaconda3/envs/jh_torch37/lib/python3.7/site-packages (from plotly>=5.0.0->dash) (1.16.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /home/ubuntu/anaconda3/envs/jh_torch37/lib/python3.7/site-packages (from plotly>=5.0.0->dash) (8.0.1)\n",
      "Requirement already satisfied: brotli in /home/ubuntu/anaconda3/envs/jh_torch37/lib/python3.7/site-packages (from flask-compress->dash) (1.0.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ubuntu/anaconda3/envs/jh_torch37/lib/python3.7/site-packages (from importlib-metadata->click>=7.1.2->Flask>=1.0.4->dash) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/ubuntu/anaconda3/envs/jh_torch37/lib/python3.7/site-packages (from importlib-metadata->click>=7.1.2->Flask>=1.0.4->dash) (3.10.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade dash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "InputGroupAddon was deprecated in dash-bootstrap-components version 1.0.0. You are using 1.0.0. For more details please see the migration guide: https://dbc-v1.herokuapp.com/migration-guide/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_51799/1628308792.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdbc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInputGroupAddon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/jh_torch37/lib/python3.7/site-packages/dash_bootstrap_components/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;31m# TODO: update URL before release\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             raise AttributeError(\n\u001b[0;32m---> 53\u001b[0;31m                 \u001b[0;34mf\"{name} was deprecated in dash-bootstrap-components version \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m                 \u001b[0;34mf\"1.0.0. You are using {__version__}. For more details please \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0;34m\"see the migration guide: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: InputGroupAddon was deprecated in dash-bootstrap-components version 1.0.0. You are using 1.0.0. For more details please see the migration guide: https://dbc-v1.herokuapp.com/migration-guide/"
     ]
    }
   ],
   "source": [
    "dbc.InputGroupAddon()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "U0HisA9GdU95",
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "InputGroupAddon was deprecated in dash-bootstrap-components version 1.0.0. You are using 1.0.0. For more details please see the migration guide: https://dbc-v1.herokuapp.com/migration-guide/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_51799/1317359877.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m     children=[\n\u001b[1;32m     53\u001b[0m         \u001b[0mdbc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"user-input\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplaceholder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Write to the chatbot...\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mdbc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInputGroupAddon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdbc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mButton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Submit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"submit\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maddon_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"append\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     ],\n\u001b[1;32m     56\u001b[0m )\n",
      "\u001b[0;32m~/anaconda3/envs/jh_torch37/lib/python3.7/site-packages/dash_bootstrap_components/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;31m# TODO: update URL before release\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             raise AttributeError(\n\u001b[0;32m---> 53\u001b[0;31m                 \u001b[0;34mf\"{name} was deprecated in dash-bootstrap-components version \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m                 \u001b[0;34mf\"1.0.0. You are using {__version__}. For more details please \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0;34m\"see the migration guide: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: InputGroupAddon was deprecated in dash-bootstrap-components version 1.0.0. You are using 1.0.0. For more details please see the migration guide: https://dbc-v1.herokuapp.com/migration-guide/"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import dash\n",
    "import dash_html_components as html\n",
    "import dash_core_components as dcc\n",
    "import dash_bootstrap_components as dbc\n",
    "from dash.dependencies import Input, Output, State\n",
    "from jupyter_dash import JupyterDash\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "def textbox(text, box=\"other\"):\n",
    "    style = {\n",
    "        \"max-width\": \"55%\",\n",
    "        \"width\": \"max-content\",\n",
    "        \"padding\": \"10px 15px\",\n",
    "        \"border-radius\": \"25px\"\n",
    "    }\n",
    "\n",
    "    if box == \"self\":\n",
    "        style[\"margin-left\"] = \"auto\"\n",
    "        style[\"margin-right\"] = 0\n",
    "\n",
    "        color = \"primary\"\n",
    "        inverse = True\n",
    "\n",
    "    elif box == \"other\":\n",
    "        style[\"margin-left\"] = 0\n",
    "        style[\"margin-right\"] = \"auto\"\n",
    "\n",
    "        color = \"light\"\n",
    "        inverse = False\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Incorrect option for `box`.\")\n",
    "\n",
    "    return dbc.Card(text, style=style, body=True, color=color, inverse=inverse)\n",
    "\n",
    "conversation = html.Div(\n",
    "    style={\n",
    "        \"width\": \"80%\",\n",
    "        \"max-width\": \"800px\",\n",
    "        \"height\": \"70vh\",\n",
    "        \"margin\": \"auto\",\n",
    "        \"overflow-y\": \"auto\",\n",
    "    },\n",
    "    id=\"display-conversation\",\n",
    ")\n",
    "\n",
    "controls = dbc.InputGroup(\n",
    "    style={\"width\": \"80%\", \"max-width\": \"800px\", \"margin\": \"auto\"},\n",
    "    children=[\n",
    "        dbc.Input(id=\"user-input\", placeholder=\"Write to the chatbot...\", type=\"text\"),\n",
    "        dbc.InputGroupAddon(dbc.Button(\"Submit\", id=\"submit\"), addon_type=\"append\",),\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "# Define app\n",
    "app = JupyterDash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])\n",
    "server = app.server\n",
    "\n",
    "\n",
    "# Define Layout\n",
    "app.layout = dbc.Container(\n",
    "    fluid=True,\n",
    "    style={'background-image': 'url(https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile21.uf.tistory.com%2Fimage%2F99A30D4B5CB15385210EA0)'},\n",
    "    children=[\n",
    "        html.H1(\"뉴스뚝딱\"),\n",
    "        html.Hr(),\n",
    "        dcc.Store(id=\"store-conversation\", data=\"\"),\n",
    "        conversation,\n",
    "        controls\n",
    "    ],\n",
    ")\n",
    "@app.callback(\n",
    "    Output(\"display-conversation\", \"children\"), [Input(\"store-conversation\", \"data\")]\n",
    ")\n",
    "def update_display(chat_history):\n",
    "    return [\n",
    "        textbox(x, box=\"self\") if i % 2 == 0 else textbox(x, box=\"other\")\n",
    "        for i, x in enumerate(chat_history.split('<token>'))\n",
    "    ]\n",
    "\n",
    "@app.callback(\n",
    "    [Output(\"store-conversation\", \"data\"), Output(\"user-input\", \"value\")],\n",
    "    [Input(\"submit\", \"n_clicks\"), Input(\"user-input\", \"n_submit\")],\n",
    "    [State(\"user-input\", \"value\"), State(\"store-conversation\", \"data\")],\n",
    ")\n",
    "def run_chatbot(n_clicks, n_submit, user_input, chat_history):\n",
    "    if n_clicks == 0:\n",
    "        return \"\", \"\"\n",
    "\n",
    "    if user_input is None or user_input == \"\":\n",
    "        return chat_history, \"\"\n",
    "\n",
    "    bot_input_ids = News_to_input(chat_history + user_input, openapi_key)\n",
    "        \n",
    "    chat_history_ids = summary(args, bot_input_ids, -1, '', None)\n",
    "    pred_lst = list(chat_history_ids[0][:5])\n",
    "    final_text = ''\n",
    "    for i,a in enumerate(user_input.split('. ')):\n",
    "        if i in pred_lst:\n",
    "            final_text = final_text+a+'. '\n",
    "    chat_history = user_input + '<token>' +final_text\n",
    "\n",
    "    return chat_history, \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UW0GrA7Mg5kr"
   },
   "source": [
    "#6. RUN!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "i_dGcr31dBdp",
    "outputId": "27f634ca-568e-49a0-e60e-bc1e6fb643d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dash app running on:\n"
     ]
    },
    {
     "data": {
      "application/javascript": "(async (port, path, text, element) => {\n    if (!google.colab.kernel.accessAllowed) {\n      return;\n    }\n    element.appendChild(document.createTextNode(''));\n    const url = await google.colab.kernel.proxyPort(port);\n    const anchor = document.createElement('a');\n    anchor.href = url + path;\n    anchor.target = '_blank';\n    anchor.setAttribute('data-href', url + path);\n    anchor.textContent = text;\n    element.appendChild(anchor);\n  })(8050, \"/\", \"http://127.0.0.1:8050/\", window.element)",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "app.run_server(mode='external')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "3bYOoWzjYSip",
    "d3Uzgqx3cLy0",
    "ifDaOjcThpEr",
    "6nhubPG1c5zt"
   ],
   "name": "Newsdata_summarybot.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "6c817662940d8689b763a83cc5e294c4f32001062a1563fe8f0cedabde224745"
  },
  "kernelspec": {
   "display_name": "Environment (conda_jh_torch37)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
